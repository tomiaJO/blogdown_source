rm(boroughs)
rm(max_count)
rm(max_avg_price)
#handling NAs
missing_values <- as.data.table(t(london[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(london)]), keep.rownames=TRUE)
setnames(missing_values, c("variable", "NA.Count"))
missing_values[order(-NA.Count)][1:10]
#no significance
ggplot(data= london, aes(x = n_review_scores_rating, y = price)) + geom_point() + geom_smooth()
# assumption: when cleaning_fee is missing, it's actually zero
london[, cleaning_fee := ifelse(is.na(usd_cleaning_fee), 0, usd_cleaning_fee)]
#step1: drop variables with lots of NAs
london[, c("usd_cleaning_fee",
"n_review_scores_rating",
"n_reviews_per_month",
"n_days_since",
"p_host_response_rate") := NULL]
#step2: drop non-complete cases
london <- london[complete.cases(london)]
# creating log price
london[, log_price:= log(price)]
# creating a total score for dummy variables. very much simplified approach
london[, d_total := Reduce("+", .SD), .SDcols = names(london) %like% "^d_.*"]
# cleaning up factor variables / duplicated
london[, c("neighbourhood_cleansed",
"property_type",
"room_type",
"usd_price_day",
"usd_cleaning_fee",
"cancellation_policy") := NULL]
# factor_cols <- names(london)[names(london) %like% "^f_.*"]
# london[, (factor_cols) := lapply(.SD, as.factor), .SDcols = factor_cols]
head(london)
rm(missing_values)
#creating subset
kensington_chelsea <- london[f_neighbourhood_cleansed == selected]
ggplot(data= london, aes(x = log_price)) + geom_histogram()
ggplot(data= kensington_chelsea, aes(x = log_price)) + geom_histogram()
# TODO: add reasoning for log transformation. two hist, not-log, common density, log
ggplot(data= london, aes(x = n_accommodates)) + geom_histogram()
ggplot(data= london, aes(x = n_accommodates, y = log_price)) + geom_point() + geom_smooth()
ggplot(data= london, aes(x = n_beds)) + geom_histogram()
ggplot(data= london, aes(x = n_beds, y = log_price)) + geom_point() + geom_smooth()
#transform both for deminishing returns?
# f_room_type
ggplot(data= london, aes(x = f_room_type)) + geom_bar()
ggplot(data= london, aes(x = f_room_type, y = log_price)) + geom_boxplot()
# f_cancellation_policy -> surprising result. correlation with sth else?
ggplot(data= london, aes(x = f_cancellation_policy)) + geom_bar()
ggplot(data= london, aes(x = f_cancellation_policy, y = log_price)) + geom_boxplot()
#d_breakfast -> no big impact. add d_ to TOTAL?
ggplot(data= london, aes(x = d_breakfast)) + geom_bar()
ggplot(data= london, aes(x = factor(d_breakfast), y = log_price)) + geom_boxplot()
#n_number_of_reviews
ggplot(data= london, aes(x = n_number_of_reviews)) + geom_histogram()
ggplot(data= kensington_chelsea, aes(x = n_number_of_reviews, y = log_price)) + geom_point() + geom_smooth()
#n_minimum_nights --> some outliers, handle
ggplot(data= london, aes(x = n_minimum_nights)) + geom_histogram()
ggplot(data= london, aes(x = n_minimum_nights, y = log_price)) + geom_point() + geom_smooth()
#f_property_type --> some interaction with other variables, eg. room type?
ggplot(data= london, aes(x = f_property_type)) + geom_bar()
ggplot(data= london, aes(x = f_property_type, y = log_price)) + geom_boxplot()
# usd_cleaning_fee --> model non-linearity?
ggplot(data= london, aes(x = cleaning_fee)) + geom_histogram()
ggplot(data= london, aes(x = cleaning_fee, y = log_price)) + geom_point() + geom_smooth()
# d_total
ggplot(data= london, aes(x = d_total)) + geom_histogram()
ggplot(data= london, aes(x = d_total, y = log_price)) + geom_point() + geom_smooth()
training_ratio <- 0.7
set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = london[["log_price"]],
times = 1,
p = training_ratio,
list = FALSE)
london_train <- london[train_indices, ]
london_test <- london[-train_indices, ]
set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = kensington_chelsea[["log_price"]],
times = 1,
p = training_ratio,
list = FALSE)
kensington_chelsea_train <- kensington_chelsea[train_indices, ]
kensington_chelsea_test <- kensington_chelsea[-train_indices, ]
fit_control <- trainControl(method = "cv", number = 10)
#for LASSO:
#tune_grid <- data.frame("cp" = c(0.01, 0.001, 0.0001, 0.00001, 0.000001))
#base  model - just using n_accomodates
set.seed(93) #for reproducibility
model_1_london <- train(log_price ~ n_accommodates,
data = london_train,
method = "lm",
trControl = fit_control)
model_1_london$results[["RMSE"]]
require(data.table)
require(ggplot2)
require(gridExtra)
require(caret)
options(scipen = 999)
theme_set(theme_minimal())   # globally set ggplot theme
set.seed(1234)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
data <- fread("airbnb_london_workfile.csv", stringsAsFactors = TRUE, quote="", fill=FALSE)
rm(list=ls())
require(data.table)
require(ggplot2)
require(gridExtra)
require(caret)
options(scipen = 999)
theme_set(theme_minimal())   # globally set ggplot theme
set.seed(1234)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
data <- fread("airbnb_london_workfile.csv", stringsAsFactors = FALSE)
boroughs <- data[, .(count = .N,
avg_price = mean(price)),
keyby = f_neighbourhood_cleansed][order(-count)]
boroughs[, borough := factor(f_neighbourhood_cleansed,
levels = boroughs[order(count)][, f_neighbourhood_cleansed])]
boroughs[, f_neighbourhood_cleansed := NULL]
# randomly picking an area > 1000
set.seed(93) #for reproducibility
selected <- sample(boroughs[count > 1000]$borough, 1)
max_count <- boroughs[, max(count)]
max_avg_price <- boroughs[, max(avg_price)]
boroughs[borough == selected, ] #TODO: make this bold on the chart
ggplot(data = boroughs) +
geom_bar(data = boroughs[borough == selected], aes(x = borough, y = 6000), fill = "lightblue",  stat = "identity") +
geom_point(aes(x = borough, y = count, color = "# of Observations"), shape = 20, size = 2) +
geom_segment(aes(x = borough, y = count, xend = borough, yend =0, color = "# of Observations")) +
geom_point(aes(x = borough, y = avg_price * (max_count/max_avg_price), color = "Avg. Price"), shape = 4, size = 2) +
scale_y_continuous(limits = c(0,6000), sec.axis = sec_axis(~./(max_count/max_avg_price), name = "Avg. Price")) +
scale_color_manual(name = "Legend", values = c("# of Observations" = "tomato", "Avg. Price" = "darkblue")) +
guides(color=guide_legend(override.aes=list(shape=15))) +
labs(y = "# of Observations", x = "Borough") +
coord_flip()
london <- copy(data)
rm(data)
rm(boroughs)
rm(max_count)
rm(max_avg_price)
#handling NAs
missing_values <- as.data.table(t(london[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(london)]), keep.rownames=TRUE)
setnames(missing_values, c("variable", "NA.Count"))
missing_values[order(-NA.Count)][1:10]
#no significance
ggplot(data= london, aes(x = n_review_scores_rating, y = price)) + geom_point() + geom_smooth()
# assumption: when cleaning_fee is missing, it's actually zero
london[, cleaning_fee := ifelse(is.na(usd_cleaning_fee), 0, usd_cleaning_fee)]
#step1: drop variables with lots of NAs
london[, c("usd_cleaning_fee",
"n_review_scores_rating",
"n_reviews_per_month",
"n_days_since",
"p_host_response_rate") := NULL]
#step2: drop non-complete cases
london <- london[complete.cases(london)]
# creating log price
london[, log_price:= log(price)]
# creating a total score for dummy variables. very much simplified approach
london[, d_total := Reduce("+", .SD), .SDcols = names(london) %like% "^d_.*"]
# cleaning up factor variables / duplicated
london[, c("neighbourhood_cleansed",
"property_type",
"room_type",
"usd_price_day",
"usd_cleaning_fee",
"cancellation_policy") := NULL]
# cleaning up factor variables / duplicated
london[, c("neighbourhood_cleansed",
"property_type",
"room_type",
"usd_price_day",
"cancellation_policy") := NULL]
factor_cols <- names(london)[names(london) %like% "^f_.*"]
factor_cols
london[, (factor_cols) := lapply(.SD, as.factor), .SDcols = factor_cols]
head(london)
rm(missing_values)
#creating subset
kensington_chelsea <- london[f_neighbourhood_cleansed == selected]
ggplot(data= london, aes(x = log_price)) + geom_histogram()
ggplot(data= kensington_chelsea, aes(x = log_price)) + geom_histogram()
# TODO: add reasoning for log transformation. two hist, not-log, common density, log
ggplot(data= london, aes(x = n_accommodates)) + geom_histogram()
ggplot(data= london, aes(x = n_accommodates, y = log_price)) + geom_point() + geom_smooth()
ggplot(data= london, aes(x = n_beds)) + geom_histogram()
ggplot(data= london, aes(x = n_beds, y = log_price)) + geom_point() + geom_smooth()
#transform both for deminishing returns?
# f_room_type
ggplot(data= london, aes(x = f_room_type)) + geom_bar()
ggplot(data= london, aes(x = f_room_type, y = log_price)) + geom_boxplot()
# f_cancellation_policy -> surprising result. correlation with sth else?
ggplot(data= london, aes(x = f_cancellation_policy)) + geom_bar()
ggplot(data= london, aes(x = f_cancellation_policy, y = log_price)) + geom_boxplot()
#d_breakfast -> no big impact. add d_ to TOTAL?
ggplot(data= london, aes(x = d_breakfast)) + geom_bar()
ggplot(data= london, aes(x = factor(d_breakfast), y = log_price)) + geom_boxplot()
#n_number_of_reviews
ggplot(data= london, aes(x = n_number_of_reviews)) + geom_histogram()
ggplot(data= kensington_chelsea, aes(x = n_number_of_reviews, y = log_price)) + geom_point() + geom_smooth()
#n_minimum_nights --> some outliers, handle
ggplot(data= london, aes(x = n_minimum_nights)) + geom_histogram()
ggplot(data= london, aes(x = n_minimum_nights, y = log_price)) + geom_point() + geom_smooth()
#f_property_type --> some interaction with other variables, eg. room type?
ggplot(data= london, aes(x = f_property_type)) + geom_bar()
ggplot(data= london, aes(x = f_property_type, y = log_price)) + geom_boxplot()
# usd_cleaning_fee --> model non-linearity?
ggplot(data= london, aes(x = cleaning_fee)) + geom_histogram()
ggplot(data= london, aes(x = cleaning_fee, y = log_price)) + geom_point() + geom_smooth()
# d_total
ggplot(data= london, aes(x = d_total)) + geom_histogram()
ggplot(data= london, aes(x = d_total, y = log_price)) + geom_point() + geom_smooth()
training_ratio <- 0.7
set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = london[["log_price"]],
times = 1,
p = training_ratio,
list = FALSE)
london_train <- london[train_indices, ]
london_test <- london[-train_indices, ]
set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = kensington_chelsea[["log_price"]],
times = 1,
p = training_ratio,
list = FALSE)
kensington_chelsea_train <- kensington_chelsea[train_indices, ]
kensington_chelsea_test <- kensington_chelsea[-train_indices, ]
fit_control <- trainControl(method = "cv", number = 10)
#base  model - just using n_accomodates
set.seed(93) #for reproducibility
model_1_london <- train(log_price ~ n_accommodates,
data = london_train,
method = "lm",
trControl = fit_control)
model_1_london$results[["RMSE"]]
#base  model - just using n_accomodates
set.seed(93) #for reproducibility
model_1_kensington_chelsea_train <- train(log_price ~ n_accommodates,
data = kensington_chelsea_train,
method = "lm",
trControl = fit_control)
model_1_kensington_chelsea_train$results[["RMSE"]]
#simple model - using multiple predictor variables
set.seed(93) #for reproducibility
model_2_london <- train(log_price ~ n_accommodates + n_beds + n_bathrooms +
f_property_type + f_room_type + f_bed_type,
data = london_train,
method = "lm",
trControl = fit_control)
model_2_london$results[["RMSE"]]
View(london)
library(twitteR)
library(tidyverse)
library(tidytext)
#authenticate to distant service
setup_twitter_oauth(
consumer_key = Sys.getenv("TWITTER_CONSUMER_KEY"),
consumer_secret = Sys.getenv("TWITTER_CONSUMER_SECRET"),
access_token = Sys.getenv("TWITTER_ACCESS_TOKEN"),
access_secret = Sys.getenv("TWITTER_ACCESS_SECRET")
)
trump <- userTimeline('realDonaldTrump', n = 3200)
obama <- userTimeline('BarackObama', n = 3200)
raw_tweets <- bind_rows(twListToDF(trump), twListToDF(obama))
words <- raw_tweets %>%
unnest_tokens(word, text) #global, should work for hungarian as well
data("stop_words")
words <- words %>%
anti_join(stop_words, by = "word") %>%
filter(! str_detect(word, "\\d"))
words_to_ignore <- data_frame(word = c("https", "amp", "t.co"))
words <- words %>%
anti_join(words_to_ignore, by = "word")
tweets <- words %>%
group_by(screenName, id, word) %>%
summarise(contains = 1) %>%
ungroup() %>%
spread(key = word, value = contains, fill = 0) %>%
mutate(tweet_by_trump = as.integer(screenName == "realDonaldTrump")) %>%
select(-screenName, -id)
library(glmnet)
fit <- cv.glmnet(
x = tweets %>% select(-tweet_by_trump) %>% as.matrix(),
y = tweets$tweet_by_trump,
family = "binomial"
)
fit
temp <- coef(fit, s = exp(-3)) %>% as.matrix()
coefficients <- data.frame(word = row.names(temp), beta = temp[, 1])
data <- coefficients %>%
filter(beta != 0) %>%
filter(word != "(Intercept)") %>%
arrange(desc(beta)) %>%
mutate(i = row_number())
ggplot(data, aes(x = i, y = beta, fill = ifelse(beta > 0, "Trump", "Obama"))) +
geom_bar(stat = "identity", alpha = 0.75) +
scale_x_continuous(breaks = data$i, labels = data$word, minor_breaks = NULL) +
xlab("") +
ylab("Coefficient Estimate") +
coord_flip() +
scale_fill_manual(
guide = guide_legend(title = "Word typically used by:"),
values = c("#446093", "#bc3939")
) +
theme_bw() +
theme(legend.position = "top")
library(wordcloud)
words %>%
filter(screenName == "realDonaldTrump") %>%
count(word) %>%
with(wordcloud(word, n, max.words = 20))
words %>%
filter(screenName == "BarackObama") %>%
count(word) %>%
with(wordcloud(word, n, max.words = 20))
ggplot(raw_tweets, aes(x = created, y = screenName)) +
geom_jitter(width = 0) +
theme_bw() +
ylab("") +
xlab("")
install.packages(c("sf", "raster", "spData", "dplyr", "RQGIS"))
install.packages(c("sf", "raster", "spData", "dplyr", "RQGIS"))
install.packages(c("sf", "raster", "spData", "dplyr", "RQGIS"))
install.packages(c("sf", "raster", "spData", "RQGIS"))
library(sf)
data(random_points, package = "RQGIS")
class(random_points)
plot(random_points)
plot(
st_geometry(random_points),
pch = 16, cex = 2,
col = "black"
bg = "lightblue"
)
plot(
st_geometry(random_points),
pch = 16, cex = 2,
col = "black",
bg = "lightblue"
)
library(dplyr)
select(random_points, 1:2) %>%
head(2)
select(random_points) %>%
head(2) #sf works with tidyverse!
select(random_points) %>%
head(2) #sf works with tidyverse!
select(random_points, 1:2) %>%
head(2) #sf works with tidyverse!
select(random_points, 1:5) %>%
head(2) #sf works with tidyverse!
select(random_points, 1:2) %>%
head(2) #sf works with tidyverse!
# one point (a numeric vector)
p = st_point(c(1.25, 1.25))
p
# one line (a matrix consisting of at
# least two points)
mat = matrix(c(1.5, 1.5, 1.7, 1.7),
ncol = 2, byrow = TRUE)
l = st_linestring(mat)
l
# one polygon
mat = matrix(c(1, 1, 1, 2, 2, 2,
2, 1, 1, 1),
ncol = 2, byrow = TRUE)
# a list of one or more matrices
# consisting of points
poly = st_polygon(list(mat))
poly
# plot it
plot(poly)
plot(p, pch = 16, col = "blue",
cex = 2, add = TRUE)
plot(l, cex = 2, col = "orange",
lwd = 2, add = TRUE)
class(lc[[1]])
lc = random_points %>%
st_geometry
class(lc)
class(lc[[1]])
lc
## attirubte operations
dim(random_points)
str(random_points)
random_points[1:2, 1:2]
select(random_points, 1:2) %>%
slice(1:2)
random_points %>% plot()
str(random_points)
#exercise
data(random_points, package = "RQGIS")
plot(random_points$spri)
random_points %>% head()
random_points %>% filter(spri > spri) %>% plot(col = "blue", add = TRUE)
random_points %>% filter(spri > 10) %>% plot(col = "blue", add = TRUE)
#exercise
data(random_points, package = "RQGIS")
plot(random_points$spri)
random_points %>% filter(spri > 10) %>% plot(col = "blue", add = TRUE)
random_points %>% filter(spri > 10) %>% select(spri) %>% plot(col = "blue", add = TRUE)
#exercise
data(random_points, package = "RQGIS")
plot(random_points$spri)
random_points %>% filter(spri > 10) %>% select(spri) %>% plot(col = "blue", add = TRUE)
#exercise
data(random_points, package = "RQGIS")
plot(random_points$spri)
selection <- random_points %>% filter(spri > 10) %>% select(spri)
plot(selection, col = "blue", add = TRUE)
plot(selection, col = "blue")
plot(random_points$spri, add= TRUE)
#exercise
data(random_points, package = "RQGIS")
plot(random_points$spri)
selection <- random_points %>% filter(spri > 10)
plot(selection$spri, col = "blue")
plot(st_geometry(random_points$spri))
plot(st_geometry(random_points))
plot(st_geometry(selection), col = "blue", add = TRUE)
# spData makes available
# nz and nz_height
library(spData)
plot(st_geometry(nz))
plot(st_geometry(nz_height),
pch = 16, col = "red2",
cex = 2, add = TRUE)
canterbury = nz %>%
filter(Name == "Canterbury")
plot(st_geometry(canterbury))
canterbury = nz %>%
filter(Name == "Canterbury")
nz
canterbury = nz %>%
filter(REGC2017_NAME == "Canterbury")
plot(st_geometry(canterbury))
plot(st_geometry(nz_height),
cex = 2, add = TRUE)
pts = st_sfc(st_point(c(0, 1)), st_point(c(1, 1))) # create 2 points
# use the buffer function to create circles from points
circles = st_buffer(pts, dist = 1)
x = circles[1, ]
circles
pts = st_sfc(st_point(c(0, 1)), st_point(c(1, 1))) # create 2 points
# use the buffer function to create circles from points
circles = st_buffer(pts, dist = 1)
x = circles[1, ]
y = circles[2, ]
circles
circles[[1]]
x = circles[[1]]
y = circles[[[2]]
y = circles[[2]]
plot(x)
plot(y, add = TRUE)
plot(x)
plot(y, col = "orange", add = TRUE)
plot(x, col = "blue")
plot(y, col = "orange", add = TRUE)
plot(st_intersection(x, y))
plot(st_union(x, y))
us_states %>%
group_by(REGION) %>%
summarize(total_pop = sum(total_pop_15))
us_states %>%
group_by(REGION) %>%
summarize(total_pop = sum(total_pop_15)) %>%
plot()
us_states %>%
group_by(REGION) %>%
summarize(avg_pop = mean(total_pop_15)) %>%
select(avg_pop) %>%
plot()
str(nz)
?nz
plot(nz)
plot(st_geometry(nz))
plot(st_geometry(st_transform(nz, crs = 4326)))
plot(st_geometry(nz))
plot(st_intersection(x, y), col = "salmon")
par(nfrow = c(1, 2))
plot(st_geometry(nz), axes = TRUE)
plot(st_geometry(st_transform(nz, crs = 4326)), axes = TRUE)
par(mfrow = c(1, 2))
plot(st_geometry(nz), axes = TRUE)
plot(st_geometry(st_transform(nz, crs = 4326)), axes = TRUE)
library(blogdown)
setwd("C:/Users/tkonc/Desktop/blogdown/blogdown_source")
serve_site()
install.packages("blogdown")
library(blogdown)
setwd("C:/Users/tkonc/Desktop/blogdown/blogdown_source")
serve_site()
serve_site()
serve_site()
serve_site()
build_site()
install.packages("tidytext")
build_site()
serve_site()
build_site()
build_site()
build_site()
build_site()
build_site()
serve_site()
serve_site()
build_site()
build_site()
serve_site()
build_site()
serve_site()
build_site()
