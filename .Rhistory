install.packages('ggplot2')
library(ggplot2)
ggplot(diamonds, aes(cut)) + geom_bar()
?matplot
installed.packages()
library(caret)
library(party)
library(data.table)
dt <- fread('C:/Users/tkonc/Desktop/CEU/ML/20180115/wk02/lect/prediction_by_hand.R')
View(dt)
library(ggplot2)
library(lubridate)
library(plyr)
library(data.table)
library(caret)
GetBikeData <- function(filePath) {
dt <- fread(filePath)
dt$quarter <- factor(dt$season, labels = c("Q1", "Q2", "Q3", "Q4"))
dt$weather <- factor(mapvalues(dt$weather, from = c(1, 2, 3, 4), to = c("Good", "Normal", "Bad", "Very Bad")))
dt$hour    <- factor(hour(ymd_hms(dt$datetime)))
dt$times   <- as.POSIXct(strftime(ymd_hms(dt$datetime), format="%H:%M:%S"), format="%H:%M:%S")
dt$weekday <- wday(ymd_hms(dt$datetime))
return(dt)
}
bikeTrain <- GetBikeData("C:/Users/tkonc/Desktop/CEU/ML/20180115/wk02/lect/data/bike_rental/bike_rental_train.csv")
bikeTrain <- GetBikeData("C:/Users/tkonc/Desktop/CEU/ML/20180115/data/bike_rental/bike_rental_train.csv")
rm(dt)
ggplot(bikeTrain, aes(x=count)) + geom_histogram()
View(bikeTrain)
# Idea
bikeTrain[, .(count = mean(count)), by=.(quarter)]
RootMeanSquaredError <- function(real_value, predicted_value) {
return(sqrt(mean((real_value - predicted_value) ^ 2)))
}
bikeTrain$prediction_model_zero <- 0
RootMeanSquaredError(bikeTrain$count, bikeTrain$prediction_model_zero)
bikeTrain$prediction_model_200 <- 200
RootMeanSquaredError(bikeTrain$count, bikeTrain$prediction_model_200)
bikeTrain$prediction_model_median <- median(bikeTrain$count)
RootMeanSquaredError(bikeTrain$count, bikeTrain$prediction_model_median)
bikeTrain$prediction_model_mean <- mean(bikeTrain$count)
RootMeanSquaredError(bikeTrain$count, bikeTrain$prediction_model_mean)
test
ModelByMean <- function(train, test) {
new_test = copy(test)
new_test$prediction <- mean(train$count)
return(new_test$prediction)
}
bikeTrain[, .(count = mean(count)), by=.(quarter)]
# Prediction
bikeTrain[quarter == "Q1", prediction_model_by_quarter := 71.90552]
bikeTrain[quarter == "Q2", prediction_model_by_quarter := 160.94075]
bikeTrain[quarter == "Q3", prediction_model_by_quarter := 186.99487]
bikeTrain[quarter == "Q4", prediction_model_by_quarter := 154.78713]
RootMeanSquaredError(bikeTrain$count, bikeTrain$prediction_model_by_quarter)
ModelByQuarter <- function(train, test) {
new_test = copy(test)
for (q in unique(train$quarter)) {
new_test[quarter == q, prediction := mean(train[quarter == q]$count) ]
}
return(new_test$prediction)
}
RootMeanSquaredError(bikeTrain$count, ModelByQuarter(bikeTrain, bikeTrain))
set.seed(123)
n <- 30
sigma <- 0.5
x <- seq(from = 0, to = 4*pi, length = n)   # x
y <- sin(x) + rnorm(n, sd = sigma)          # y = f(x) + eps
plot(y ~ x)                                         # y ~ x
lines(sin(x) ~ x, col = "red")                      # f(x) ~ x
deg <- 20    # try: 5, 4, 8, 20   (complexity of the model)
dpoly_xy <- data.frame(poly(x,deg), y)      # expand x poly
md <- lm(y ~ ., dpoly_xy)                   # f^
lines(predict(md, dpoly_xy) ~ x, col = "blue")         # f^(x) ~ x
lines(predict(md, dpoly_xy) ~ x, col = "#5555ff")      # f^(x) ~ x
lines(predict(md, dpoly_xy) ~ x, col = "magenta")      # f^(x) ~ x
lines(predict(md, dpoly_xy) ~ x, col = "#aaaaff")      # f^(x) ~ x
require(data.table)
require(ggplot2)
require(gridExtra)
require(caret)
options(scipen = 999)
theme_set(theme_minimal())   # globally set ggplot theme
set.seed(1234)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
data <- fread("airbnb_london_workfile.csv", stringsAsFactors = TRUE)
boroughs <- data[, .(count = .N,
avg_price = mean(price)),
keyby = f_neighbourhood_cleansed][order(-count)]
boroughs[, borough := factor(f_neighbourhood_cleansed,
levels = boroughs[order(count)][, f_neighbourhood_cleansed])]
boroughs[, f_neighbourhood_cleansed := NULL]
# randomly picking an area > 1000
set.seed(93) #for reproducibility
selected <- sample(boroughs[count > 1000]$borough, 1)
max_count <- boroughs[, max(count)]
max_avg_price <- boroughs[, max(avg_price)]
boroughs[borough == selected, ] #TODO: make this bold on the chart
ggplot(data = boroughs) +
geom_bar(data = boroughs[borough == selected], aes(x = borough, y = 6000), fill = "lightblue",  stat = "identity") +
geom_point(aes(x = borough, y = count, color = "# of Observations"), shape = 20, size = 2) +
geom_segment(aes(x = borough, y = count, xend = borough, yend =0, color = "# of Observations")) +
geom_point(aes(x = borough, y = avg_price * (max_count/max_avg_price), color = "Avg. Price"), shape = 4, size = 2) +
scale_y_continuous(limits = c(0,6000), sec.axis = sec_axis(~./(max_count/max_avg_price), name = "Avg. Price")) +
scale_color_manual(name = "Legend", values = c("# of Observations" = "tomato", "Avg. Price" = "darkblue")) +
guides(color=guide_legend(override.aes=list(shape=15))) +
labs(y = "# of Observations", x = "Borough") +
coord_flip()
london <- copy(data)
rm(data)
rm(boroughs)
rm(max_count)
rm(max_avg_price)
# creating log price
london[, log_price:= log(price)]
# creating a total score for dummy variables. very much simplified approach
london[, d_total := Reduce("+", .SD), .SDcols = names(london) %like% "^d_.*"]
# cleaning up factor variables
london[, c("neighbourhood_cleansed",
"property_type",
"room_type",
"usd_price_day",
"usd_cleaning_fee",
"cancellation_policy") := NULL]
# factor_cols <- names(london)[names(london) %like% "^f_.*"]
# london[, (factor_cols) := lapply(.SD, as.factor), .SDcols = factor_cols]
head(london)
#handling NAs
missing_values <- as.data.table(t(london[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(london)]), keep.rownames=TRUE)
setnames(missing_values, c("variable", "NA.Count"))
missing_values[order(-NA.Count)][1:10]
#no significance
ggplot(data= london, aes(x = n_review_scores_rating, y = log_price)) + geom_point() + geom_smooth()
# assumption: when cleaning_fee is missing, it's actually zero
london[, cleaning_fee := ifelse(is.na(usd_cleaning_fee), 0, usd_cleaning_fee)]
data <- fread("airbnb_london_workfile.csv", stringsAsFactors = TRUE)
boroughs <- data[, .(count = .N,
avg_price = mean(price)),
keyby = f_neighbourhood_cleansed][order(-count)]
boroughs[, borough := factor(f_neighbourhood_cleansed,
levels = boroughs[order(count)][, f_neighbourhood_cleansed])]
boroughs[, f_neighbourhood_cleansed := NULL]
# randomly picking an area > 1000
set.seed(93) #for reproducibility
selected <- sample(boroughs[count > 1000]$borough, 1)
max_count <- boroughs[, max(count)]
max_avg_price <- boroughs[, max(avg_price)]
boroughs[borough == selected, ] #TODO: make this bold on the chart
ggplot(data = boroughs) +
geom_bar(data = boroughs[borough == selected], aes(x = borough, y = 6000), fill = "lightblue",  stat = "identity") +
geom_point(aes(x = borough, y = count, color = "# of Observations"), shape = 20, size = 2) +
geom_segment(aes(x = borough, y = count, xend = borough, yend =0, color = "# of Observations")) +
geom_point(aes(x = borough, y = avg_price * (max_count/max_avg_price), color = "Avg. Price"), shape = 4, size = 2) +
scale_y_continuous(limits = c(0,6000), sec.axis = sec_axis(~./(max_count/max_avg_price), name = "Avg. Price")) +
scale_color_manual(name = "Legend", values = c("# of Observations" = "tomato", "Avg. Price" = "darkblue")) +
guides(color=guide_legend(override.aes=list(shape=15))) +
labs(y = "# of Observations", x = "Borough") +
coord_flip()
london <- copy(data)
rm(data)
rm(boroughs)
rm(max_count)
rm(max_avg_price)
#handling NAs
missing_values <- as.data.table(t(london[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(london)]), keep.rownames=TRUE)
setnames(missing_values, c("variable", "NA.Count"))
missing_values[order(-NA.Count)][1:10]
#no significance
ggplot(data= london, aes(x = n_review_scores_rating, y = price)) + geom_point() + geom_smooth()
# assumption: when cleaning_fee is missing, it's actually zero
london[, cleaning_fee := ifelse(is.na(usd_cleaning_fee), 0, usd_cleaning_fee)]
#step1: drop variables with lots of NAs
london[, c("usd_cleaning_fee",
"n_review_scores_rating",
"n_reviews_per_month",
"n_days_since",
"p_host_response_rate") := NULL]
#step2: drop non-complete cases
london <- london[complete.cases(london)]
# creating log price
london[, log_price:= log(price)]
# creating a total score for dummy variables. very much simplified approach
london[, d_total := Reduce("+", .SD), .SDcols = names(london) %like% "^d_.*"]
# cleaning up factor variables / duplicated
london[, c("neighbourhood_cleansed",
"property_type",
"room_type",
"usd_price_day",
"usd_cleaning_fee",
"cancellation_policy") := NULL]
# factor_cols <- names(london)[names(london) %like% "^f_.*"]
# london[, (factor_cols) := lapply(.SD, as.factor), .SDcols = factor_cols]
head(london)
rm(missing_values)
#creating subset
kensington_chelsea <- london[f_neighbourhood_cleansed == selected]
ggplot(data= london, aes(x = log_price)) + geom_histogram()
ggplot(data= kensington_chelsea, aes(x = log_price)) + geom_histogram()
# TODO: add reasoning for log transformation. two hist, not-log, common density, log
ggplot(data= london, aes(x = n_accommodates)) + geom_histogram()
ggplot(data= london, aes(x = n_accommodates, y = log_price)) + geom_point() + geom_smooth()
ggplot(data= london, aes(x = n_beds)) + geom_histogram()
ggplot(data= london, aes(x = n_beds, y = log_price)) + geom_point() + geom_smooth()
#transform both for deminishing returns?
# f_room_type
ggplot(data= london, aes(x = f_room_type)) + geom_bar()
ggplot(data= london, aes(x = f_room_type, y = log_price)) + geom_boxplot()
# f_cancellation_policy -> surprising result. correlation with sth else?
ggplot(data= london, aes(x = f_cancellation_policy)) + geom_bar()
ggplot(data= london, aes(x = f_cancellation_policy, y = log_price)) + geom_boxplot()
#d_breakfast -> no big impact. add d_ to TOTAL?
ggplot(data= london, aes(x = d_breakfast)) + geom_bar()
ggplot(data= london, aes(x = factor(d_breakfast), y = log_price)) + geom_boxplot()
#n_number_of_reviews
ggplot(data= london, aes(x = n_number_of_reviews)) + geom_histogram()
ggplot(data= kensington_chelsea, aes(x = n_number_of_reviews, y = log_price)) + geom_point() + geom_smooth()
#n_minimum_nights --> some outliers, handle
ggplot(data= london, aes(x = n_minimum_nights)) + geom_histogram()
ggplot(data= london, aes(x = n_minimum_nights, y = log_price)) + geom_point() + geom_smooth()
#f_property_type --> some interaction with other variables, eg. room type?
ggplot(data= london, aes(x = f_property_type)) + geom_bar()
ggplot(data= london, aes(x = f_property_type, y = log_price)) + geom_boxplot()
# usd_cleaning_fee --> model non-linearity?
ggplot(data= london, aes(x = cleaning_fee)) + geom_histogram()
ggplot(data= london, aes(x = cleaning_fee, y = log_price)) + geom_point() + geom_smooth()
# d_total
ggplot(data= london, aes(x = d_total)) + geom_histogram()
ggplot(data= london, aes(x = d_total, y = log_price)) + geom_point() + geom_smooth()
training_ratio <- 0.7
set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = london[["log_price"]],
times = 1,
p = training_ratio,
list = FALSE)
london_train <- london[train_indices, ]
london_test <- london[-train_indices, ]
set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = kensington_chelsea[["log_price"]],
times = 1,
p = training_ratio,
list = FALSE)
kensington_chelsea_train <- kensington_chelsea[train_indices, ]
kensington_chelsea_test <- kensington_chelsea[-train_indices, ]
fit_control <- trainControl(method = "cv", number = 10)
#for LASSO:
#tune_grid <- data.frame("cp" = c(0.01, 0.001, 0.0001, 0.00001, 0.000001))
#base  model - just using n_accomodates
set.seed(93) #for reproducibility
model_1_london <- train(log_price ~ n_accommodates,
data = london_train,
method = "lm",
trControl = fit_control)
model_1_london$results[["RMSE"]]
require(data.table)
require(ggplot2)
require(gridExtra)
require(caret)
options(scipen = 999)
theme_set(theme_minimal())   # globally set ggplot theme
set.seed(1234)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
data <- fread("airbnb_london_workfile.csv", stringsAsFactors = TRUE, quote="", fill=FALSE)
rm(list=ls())
require(data.table)
require(ggplot2)
require(gridExtra)
require(caret)
options(scipen = 999)
theme_set(theme_minimal())   # globally set ggplot theme
set.seed(1234)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
data <- fread("airbnb_london_workfile.csv", stringsAsFactors = FALSE)
boroughs <- data[, .(count = .N,
avg_price = mean(price)),
keyby = f_neighbourhood_cleansed][order(-count)]
boroughs[, borough := factor(f_neighbourhood_cleansed,
levels = boroughs[order(count)][, f_neighbourhood_cleansed])]
boroughs[, f_neighbourhood_cleansed := NULL]
# randomly picking an area > 1000
set.seed(93) #for reproducibility
selected <- sample(boroughs[count > 1000]$borough, 1)
max_count <- boroughs[, max(count)]
max_avg_price <- boroughs[, max(avg_price)]
boroughs[borough == selected, ] #TODO: make this bold on the chart
ggplot(data = boroughs) +
geom_bar(data = boroughs[borough == selected], aes(x = borough, y = 6000), fill = "lightblue",  stat = "identity") +
geom_point(aes(x = borough, y = count, color = "# of Observations"), shape = 20, size = 2) +
geom_segment(aes(x = borough, y = count, xend = borough, yend =0, color = "# of Observations")) +
geom_point(aes(x = borough, y = avg_price * (max_count/max_avg_price), color = "Avg. Price"), shape = 4, size = 2) +
scale_y_continuous(limits = c(0,6000), sec.axis = sec_axis(~./(max_count/max_avg_price), name = "Avg. Price")) +
scale_color_manual(name = "Legend", values = c("# of Observations" = "tomato", "Avg. Price" = "darkblue")) +
guides(color=guide_legend(override.aes=list(shape=15))) +
labs(y = "# of Observations", x = "Borough") +
coord_flip()
london <- copy(data)
rm(data)
rm(boroughs)
rm(max_count)
rm(max_avg_price)
#handling NAs
missing_values <- as.data.table(t(london[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(london)]), keep.rownames=TRUE)
setnames(missing_values, c("variable", "NA.Count"))
missing_values[order(-NA.Count)][1:10]
#no significance
ggplot(data= london, aes(x = n_review_scores_rating, y = price)) + geom_point() + geom_smooth()
# assumption: when cleaning_fee is missing, it's actually zero
london[, cleaning_fee := ifelse(is.na(usd_cleaning_fee), 0, usd_cleaning_fee)]
#step1: drop variables with lots of NAs
london[, c("usd_cleaning_fee",
"n_review_scores_rating",
"n_reviews_per_month",
"n_days_since",
"p_host_response_rate") := NULL]
#step2: drop non-complete cases
london <- london[complete.cases(london)]
# creating log price
london[, log_price:= log(price)]
# creating a total score for dummy variables. very much simplified approach
london[, d_total := Reduce("+", .SD), .SDcols = names(london) %like% "^d_.*"]
# cleaning up factor variables / duplicated
london[, c("neighbourhood_cleansed",
"property_type",
"room_type",
"usd_price_day",
"usd_cleaning_fee",
"cancellation_policy") := NULL]
# cleaning up factor variables / duplicated
london[, c("neighbourhood_cleansed",
"property_type",
"room_type",
"usd_price_day",
"cancellation_policy") := NULL]
factor_cols <- names(london)[names(london) %like% "^f_.*"]
factor_cols
london[, (factor_cols) := lapply(.SD, as.factor), .SDcols = factor_cols]
head(london)
rm(missing_values)
#creating subset
kensington_chelsea <- london[f_neighbourhood_cleansed == selected]
ggplot(data= london, aes(x = log_price)) + geom_histogram()
ggplot(data= kensington_chelsea, aes(x = log_price)) + geom_histogram()
# TODO: add reasoning for log transformation. two hist, not-log, common density, log
ggplot(data= london, aes(x = n_accommodates)) + geom_histogram()
ggplot(data= london, aes(x = n_accommodates, y = log_price)) + geom_point() + geom_smooth()
ggplot(data= london, aes(x = n_beds)) + geom_histogram()
ggplot(data= london, aes(x = n_beds, y = log_price)) + geom_point() + geom_smooth()
#transform both for deminishing returns?
# f_room_type
ggplot(data= london, aes(x = f_room_type)) + geom_bar()
ggplot(data= london, aes(x = f_room_type, y = log_price)) + geom_boxplot()
# f_cancellation_policy -> surprising result. correlation with sth else?
ggplot(data= london, aes(x = f_cancellation_policy)) + geom_bar()
ggplot(data= london, aes(x = f_cancellation_policy, y = log_price)) + geom_boxplot()
#d_breakfast -> no big impact. add d_ to TOTAL?
ggplot(data= london, aes(x = d_breakfast)) + geom_bar()
ggplot(data= london, aes(x = factor(d_breakfast), y = log_price)) + geom_boxplot()
#n_number_of_reviews
ggplot(data= london, aes(x = n_number_of_reviews)) + geom_histogram()
ggplot(data= kensington_chelsea, aes(x = n_number_of_reviews, y = log_price)) + geom_point() + geom_smooth()
#n_minimum_nights --> some outliers, handle
ggplot(data= london, aes(x = n_minimum_nights)) + geom_histogram()
ggplot(data= london, aes(x = n_minimum_nights, y = log_price)) + geom_point() + geom_smooth()
#f_property_type --> some interaction with other variables, eg. room type?
ggplot(data= london, aes(x = f_property_type)) + geom_bar()
ggplot(data= london, aes(x = f_property_type, y = log_price)) + geom_boxplot()
# usd_cleaning_fee --> model non-linearity?
ggplot(data= london, aes(x = cleaning_fee)) + geom_histogram()
ggplot(data= london, aes(x = cleaning_fee, y = log_price)) + geom_point() + geom_smooth()
# d_total
ggplot(data= london, aes(x = d_total)) + geom_histogram()
ggplot(data= london, aes(x = d_total, y = log_price)) + geom_point() + geom_smooth()
training_ratio <- 0.7
set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = london[["log_price"]],
times = 1,
p = training_ratio,
list = FALSE)
london_train <- london[train_indices, ]
london_test <- london[-train_indices, ]
set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = kensington_chelsea[["log_price"]],
times = 1,
p = training_ratio,
list = FALSE)
kensington_chelsea_train <- kensington_chelsea[train_indices, ]
kensington_chelsea_test <- kensington_chelsea[-train_indices, ]
fit_control <- trainControl(method = "cv", number = 10)
#base  model - just using n_accomodates
set.seed(93) #for reproducibility
model_1_london <- train(log_price ~ n_accommodates,
data = london_train,
method = "lm",
trControl = fit_control)
model_1_london$results[["RMSE"]]
#base  model - just using n_accomodates
set.seed(93) #for reproducibility
model_1_kensington_chelsea_train <- train(log_price ~ n_accommodates,
data = kensington_chelsea_train,
method = "lm",
trControl = fit_control)
model_1_kensington_chelsea_train$results[["RMSE"]]
#simple model - using multiple predictor variables
set.seed(93) #for reproducibility
model_2_london <- train(log_price ~ n_accommodates + n_beds + n_bathrooms +
f_property_type + f_room_type + f_bed_type,
data = london_train,
method = "lm",
trControl = fit_control)
model_2_london$results[["RMSE"]]
View(london)
library(twitteR)
library(tidyverse)
library(tidytext)
#authenticate to distant service
setup_twitter_oauth(
consumer_key = Sys.getenv("TWITTER_CONSUMER_KEY"),
consumer_secret = Sys.getenv("TWITTER_CONSUMER_SECRET"),
access_token = Sys.getenv("TWITTER_ACCESS_TOKEN"),
access_secret = Sys.getenv("TWITTER_ACCESS_SECRET")
)
trump <- userTimeline('realDonaldTrump', n = 3200)
obama <- userTimeline('BarackObama', n = 3200)
raw_tweets <- bind_rows(twListToDF(trump), twListToDF(obama))
words <- raw_tweets %>%
unnest_tokens(word, text) #global, should work for hungarian as well
data("stop_words")
words <- words %>%
anti_join(stop_words, by = "word") %>%
filter(! str_detect(word, "\\d"))
words_to_ignore <- data_frame(word = c("https", "amp", "t.co"))
words <- words %>%
anti_join(words_to_ignore, by = "word")
tweets <- words %>%
group_by(screenName, id, word) %>%
summarise(contains = 1) %>%
ungroup() %>%
spread(key = word, value = contains, fill = 0) %>%
mutate(tweet_by_trump = as.integer(screenName == "realDonaldTrump")) %>%
select(-screenName, -id)
library(glmnet)
fit <- cv.glmnet(
x = tweets %>% select(-tweet_by_trump) %>% as.matrix(),
y = tweets$tweet_by_trump,
family = "binomial"
)
fit
temp <- coef(fit, s = exp(-3)) %>% as.matrix()
coefficients <- data.frame(word = row.names(temp), beta = temp[, 1])
data <- coefficients %>%
filter(beta != 0) %>%
filter(word != "(Intercept)") %>%
arrange(desc(beta)) %>%
mutate(i = row_number())
ggplot(data, aes(x = i, y = beta, fill = ifelse(beta > 0, "Trump", "Obama"))) +
geom_bar(stat = "identity", alpha = 0.75) +
scale_x_continuous(breaks = data$i, labels = data$word, minor_breaks = NULL) +
xlab("") +
ylab("Coefficient Estimate") +
coord_flip() +
scale_fill_manual(
guide = guide_legend(title = "Word typically used by:"),
values = c("#446093", "#bc3939")
) +
theme_bw() +
theme(legend.position = "top")
library(wordcloud)
words %>%
filter(screenName == "realDonaldTrump") %>%
count(word) %>%
with(wordcloud(word, n, max.words = 20))
words %>%
filter(screenName == "BarackObama") %>%
count(word) %>%
with(wordcloud(word, n, max.words = 20))
ggplot(raw_tweets, aes(x = created, y = screenName)) +
geom_jitter(width = 0) +
theme_bw() +
ylab("") +
xlab("")
library(blogdown)
setwd("C:/Users/tkonc/Desktop/blogdown/blogdown_source")
serve_site()
serve_site()
serve_site()
serve_site()
build_site()
